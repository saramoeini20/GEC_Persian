{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kiMdL-EAIqyuNY-WVsxCeA3vEO9Ps93R",
      "authorship_tag": "ABX9TyOHT+tp4qMYYQjvW/YH9gOE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saramoeini20/GEC_Persian/blob/main/GenerateError.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        },
        "id": "BjCTct2D4V8O",
        "outputId": "ab06a6db-e3c4-4728-e770-106799bd8524"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hazm\n",
            "  Downloading hazm-0.9.3-py3-none-any.whl (367 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.5/367.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Collecting numpy<2.0.0,>=1.24.3 (from hazm)\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel<0.10.0,>=0.9.2->hazm)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.2.0)\n",
            "Installing collected packages: python-crfsuite, pybind11, numpy, fasttext-wheel, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.2 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 hazm-0.9.3 numpy-1.25.2 pybind11-2.11.1 python-crfsuite-0.9.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hazm import *\n",
        "import ast"
      ],
      "metadata": {
        "id": "Sjxpb2z-4Yyt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = Stemmer()\n"
      ],
      "metadata": {
        "id": "YAdD1pt54f-4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ArabicPlu(list_line):\n",
        "  flag = 0\n",
        "  new_line = list_line\n",
        "  for i in range(0,len(new_line)):\n",
        "    if new_line[i][1] == 'NOUN,EZ' or new_line[i][1] == 'NOUN':\n",
        "        stem_of_word = stemmer.stem(new_line[i][0])\n",
        "        if stem_of_word != '':\n",
        "          suff = new_line[i][0].split(stem_of_word)[-1]\n",
        "          if suff in ['ان','‌ها','ها','انی','های','‌های','هایی','‌هایی']:\n",
        "            flag = 1\n",
        "            new_word = stem_of_word + 'ات'\n",
        "            if suff in ['انی','های','‌های','هایی','‌هایی']:\n",
        "              new_word = new_word + 'ی'\n",
        "            new_line = new_line [:i] + [(new_word , new_line [i][1])] + new_line[i+1:]\n",
        "  if flag == 1:\n",
        "    # f2.write(' '.join(str(e[0]) for e in new_line))\n",
        "    # f2.write('\\n')\n",
        "    # f3.write(' '.join(str(e[0]) for e in ast.literal_eval(line)))\n",
        "    # f3.write('\\n')\n",
        "    return ' '.join(str(e[0]) for e in new_line)\n",
        "\n"
      ],
      "metadata": {
        "id": "DV6anHE-xnRN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_3F8KTkq3JY"
      },
      "source": [
        "##که زائد :  'که' بعد از 'آنچه' نباید به کار رود.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#AncheKe\n",
        "def Redundancy_1(list_line):\n",
        "  flag = 0\n",
        "  for i in range(0,len(list_line)-2):\n",
        "    if list_line[i][0] == 'آنچه':\n",
        "      if list_line[i+1][0] != 'را':\n",
        "        new_line = list_line[0:i+1] + [('که','SCONJ')] + list_line[i+1:]\n",
        "        list_line = new_line\n",
        "        flag = 1\n",
        "\n",
        "        #باقی گذاشتن را\n",
        "      elif list_line[i+1][0] == 'را':\n",
        "        new_line = list_line[0:i+2] + [('که','SCONJ')] + list_line[i+2:]\n",
        "        list_line = new_line\n",
        "        flag = 1\n",
        "  if flag == 1:\n",
        "    f3.write(' '.join(str(e[0]) for e in ast.literal_eval(line)))\n",
        "    f3.write('\\n')\n",
        "    f2.write(' '.join(str(e[0]) for e in list_line))\n",
        "    f2.write('\\n')\n",
        "  # return ' '.join(str(e[0]) for e in list_line)\n"
      ],
      "metadata": {
        "id": "4eC9tYXp5cpc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKO6tpO9ig77"
      },
      "source": [
        "##اما زائد :  'اما' بعد از 'اگرچه' نباید به کار رود.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#AgarcheAmma\n",
        "def Redundancy_2(list_line):\n",
        "  flag = 0\n",
        "  indexes_ACh = [i for i, tup in enumerate(list_line) if tup[0] == 'اگرچه']\n",
        "  for index_Ach in indexes_ACh:\n",
        "    for i in range(index_Ach,len(list_line)-2):\n",
        "      if list_line[i+1][1] != 'VERB':\n",
        "\n",
        "        continue\n",
        "      elif list_line[i+1][1] == 'VERB' and list_line[i+2][1] != 'CCONJ' and list_line[i+2][1] != 'SCONJ':\n",
        "        new_line = list_line[0:i+2] + [('اما','CCONJ')] + list_line[i+2:]\n",
        "        list_line = new_line\n",
        "        flag = 1\n",
        "        break\n",
        "\n",
        "  if flag == 1:\n",
        "    f3.write(' '.join(str(e[0]) for e in ast.literal_eval(line)))\n",
        "    f3.write('\\n')\n",
        "    f2.write(' '.join(str(e[0]) for e in list_line))\n",
        "    f2.write('\\n')\n",
        "  # return ' '.join(str(e[0]) for e in list_line)\n"
      ],
      "metadata": {
        "id": "oZZmaqFP6JJU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M4wb7wWxWIbq"
      },
      "outputs": [],
      "source": [
        "#ماضی نقلی\n",
        "Suff_1 = ['ایم','اید']\n",
        "Suff_2 = ['ایم','ام']\n",
        "Suff_3 = ['اید','ای']\n",
        "#بقیه\n",
        "Suff_4 = ['م','یم']\n",
        "Suff_5 = ['ند','ید']\n",
        "Suff_6 = ['یم','ید']\n",
        "\n",
        "Suffix = [Suff_1,Suff_2,Suff_3,Suff_4,Suff_5,Suff_6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H80JqvP1YW4I"
      },
      "outputs": [],
      "source": [
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "def SVA(list_line):\n",
        "  flag = 0\n",
        "  new_line = list_line\n",
        "  for i in range (0,len(list_line)):\n",
        "    suff_list = []\n",
        "    if list_line[i][1] == 'VERB':\n",
        "      if list_line[i][0] == lemmatizer.lemmatize(list_line[i][0]).split('#')[0] :\n",
        "        continue\n",
        "      if list_line[i][0][0:4] != 'خواه' and list_line[i][0][0:5] != 'نخواه':\n",
        "\n",
        "        lems_of_verb = lemmatizer.lemmatize(list_line[i][0])\n",
        "        root = lems_of_verb.split('#')\n",
        "        for item in root:\n",
        "          if item != '' and item in list_line[i][0]:\n",
        "\n",
        "            for suff in Suffix:\n",
        "\n",
        "              if list_line[i][0].split(item)[-1] in suff:\n",
        "                new_suff = [x for x in suff if x != list_line[i][0].split(item)[-1] ]\n",
        "                # suff_list.append([x for x in new_suff])\n",
        "\n",
        "\n",
        "                # list_line = new_line\n",
        "                flag = 1\n",
        "                # break\n",
        "            if flag == 1:\n",
        "\n",
        "              new_verb = new_line[i][0].removesuffix(new_line[i][0].split(item)[-1])+ new_suff[0]\n",
        "              new_line = new_line[0:i] + [(new_verb,'VERB')] + new_line[i+1:]\n",
        "\n",
        "      if list_line[i][0][0:4] == 'خواه' or list_line[i][0][0:5] == 'نخواه':\n",
        "\n",
        "        for suff in Suffix:\n",
        "          if list_line[i][0][0:4] == 'خواه':\n",
        "            if list_line[i][0].split('_')[0].split('خواه')[1] in suff:\n",
        "              new_suff = [x for x in suff if x != list_line[i][0].split('_')[0].split('خواه')[1] ]\n",
        "              # new_verb = 'خواه' + new_suff[0] + '_' + list_line[i][0].split('_')[1]\n",
        "              # new_line = new_line[0:i] + [(new_verb,'VERB')] + new_line[i+1:]\n",
        "              # new_line = new_line\n",
        "              flag = 1\n",
        "              # break\n",
        "\n",
        "          elif list_line[i][0][0:5] == 'نخواه':\n",
        "            if list_line[i][0].split('_')[0].split('نخواه')[1] in suff:\n",
        "              new_suff = [x for x in suff if x != list_line[i][0].split('_')[0].split('نخواه')[1] ]\n",
        "              # new_verb = 'نخواه' + new_suff[0] + '_' + list_line[i][0].split('_')[1]\n",
        "              flag = 1\n",
        "              # break\n",
        "\n",
        "\n",
        "        if flag == 1:\n",
        "          if list_line[i][0][0:4] == 'خواه':\n",
        "            ###random\n",
        "            new_verb = 'خواه' + new_suff[0] + '_' + list_line[i][0].split('_')[1]\n",
        "            new_line = new_line[0:i] + [(new_verb,'VERB')] + new_line[i+1:]\n",
        "\n",
        "          if list_line[i][0][0:5] == 'نخواه':\n",
        "            new_verb = 'نخواه' + new_suff[0] + '_' + list_line[i][0].split('_')[1]\n",
        "            new_line = new_line[0:i] + [(new_verb,'VERB')] + new_line[i+1:]\n",
        "\n",
        "  # if flag == 1:\n",
        "  #   f2.write(' '.join(str(e[0]) for e in new_line))\n",
        "  #   f2.write('\\n')\n",
        "  #   f3.write(' '.join(str(e[0]) for e in ast.literal_eval(line)))\n",
        "  #   f3.write('\\n')\n",
        "  return ' '.join(str(e[0]) for e in new_line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### نشانه مفعول حرف اضافه 'را' است که معمولا این 'را' به اشتباه بلافاصله بعد از مفعول قرار نمی‌گیرد و در مکان‌های دیگری ظاهر می‌شود. در ادامه پرکاربردترین این مکان‌ها را به عنوان خطا می‌سازیم."
      ],
      "metadata": {
        "id": "yev1_7kPc-9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7Vs_c6w9naMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  ####      سودان استرداد نمیری از مصر را تقاضا می‌کند. -----> غلط\n",
        "  ####       سودان استرداد نمیری را از مصر تقاضا می‌کند.    ----->   درست"
      ],
      "metadata": {
        "id": "S9tSTHscm-uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Type_1_CLITIC(indexes_CLITIC,list_line):\n",
        "  new_line = list_line\n",
        "  flag = 0\n",
        "  for index_clitic in indexes_CLITIC:\n",
        "\n",
        "    if new_line[index_clitic+1][1] == 'ADP':\n",
        "      for i in range(index_clitic+2 ,len(new_line)):\n",
        "        if new_line[i][1] == 'NOUN' and new_line[i-1][0] != 'که':\n",
        "          new_line = new_line[0:index_clitic] + new_line[index_clitic+1:i+1] + [new_line[index_clitic]] + new_line[i+1:len(new_line)+1]\n",
        "          flag = 1\n",
        "          break\n",
        "        elif new_line[i][1] == 'NOUN,EZ' or new_line[i][1] == 'ADJ,EZ':\n",
        "          continue\n",
        "  if flag == 1:\n",
        "    f3.write(' '.join(str(e[0]) for e in list_line))\n",
        "    f3.write('\\n')\n",
        "    f2.write(' '.join(str(e[0]) for e in new_line))\n",
        "    f2.write('\\n')\n",
        "  # return ' '.join(str(e[0]) for e in new_line)\n"
      ],
      "metadata": {
        "id": "SHtjoDqY92m_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####گلهایی که مادر خریده بود را در گلدان گذاشت    ----->    غلط\n",
        "####گلهایی را که مادر خریده بود در گلدان گذاشت    ----->   درست"
      ],
      "metadata": {
        "id": "NNupMfsMnUxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prepo = ['CCONJ','SCONJ']\n",
        "def Type_2_CLITIC(indexes_CLITIC,list_line):\n",
        "  new_line = list_line\n",
        "  flag = 0\n",
        "  for index_clitic in indexes_CLITIC:\n",
        "\n",
        "    if new_line[index_clitic+1][1] in prepo :\n",
        "      for i in range(index_clitic+2 ,len(new_line)):\n",
        "        if new_line[i][1] == 'VERB':\n",
        "          new_line = new_line[0:index_clitic] + new_line[index_clitic+1:i+1] + [new_line[index_clitic]] + new_line[i+1:len(new_line)+1]\n",
        "          flag = 1\n",
        "          break\n",
        "  if flag == 1:\n",
        "    f3.write(' '.join(str(e[0]) for e in list_line))\n",
        "    f3.write('\\n')\n",
        "    f2.write(' '.join(str(e[0]) for e in new_line))\n",
        "    f2.write('\\n')\n",
        "  # return ' '.join(str(e[0]) for e in new_line)\n"
      ],
      "metadata": {
        "id": "DJf46kFn95PS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Source and Target file"
      ],
      "metadata": {
        "id": "EqUWwgqYmzCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/GEC/Data/News_POS\",'r') as f1,open(\"/content/drive/MyDrive/GEC/Data/Errors/Source\",'w') as f2,open(\"/content/drive/MyDrive/GEC/Data/Errors/Target\",'w') as f3:\n",
        "    for line in f1:\n",
        "      list_line = ast.literal_eval(line)\n",
        "      ArabicPlu (list_line)\n",
        "      Redundancy_1(list_line)\n",
        "      Redundancy_2(list_line)\n",
        "\n",
        "      indexes_CLITIC = [i for i, tup in enumerate(list_line) if tup[0] == 'را']\n",
        "      Type_1_CLITIC(indexes_CLITIC,list_line)\n",
        "      Type_2_CLITIC(indexes_CLITIC,list_line)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7S6HtLRHw0YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test random"
      ],
      "metadata": {
        "id": "pLdutFsPnkqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Sentence =\"وزارت دادگستری نشستی را به همکاری کمیته بین‌المللی صلیب سرخ در کابل برگزار کرده که در آن مسئولان بازداشتگاه‌های ۳۴ ولایت افغانستان شرکت کرده‌اند. \""
      ],
      "metadata": {
        "id": "wWocA3wCuqid"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Sent_pos=[('وزارت', 'NOUN,EZ'), ('دادگستری', 'NOUN'), ('نشستی', 'NOUN'), ('را', 'ADP'), ('به', 'ADP'), ('همکاری', 'NOUN,EZ'), ('کمیته', 'NOUN,EZ'), ('بین\\u200cالمللی', 'ADJ,EZ'), ('صلیب', 'NOUN,EZ'), ('سرخ', 'ADJ'), ('در', 'ADP'), ('کابل', 'NOUN'), ('برگزار', 'ADJ'), ('کرده', 'VERB'), ('که', 'SCONJ'), ('در', 'ADP'), ('آن', 'PRON'), ('مسئولان', 'NOUN,EZ'), ('بازداشتگاه\\u200cهای', 'NOUN,EZ'), ('۳۴', 'NUM'), ('ولایت', 'NOUN,EZ'), ('افغانستان', 'NOUN'), ('شرکت', 'NOUN'), ('کرده\\u200cاند', 'VERB'), ('.', 'PUNCT')]"
      ],
      "metadata": {
        "id": "imvwDcJeu_pb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ArabicPlu(Sent_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rj__xYh5e8Dn",
        "outputId": "3d769ac9-c8bb-4c0d-f8b5-fa5a6b89c7fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'وزارت دادگستری نشستی را به همکاری کمیته بین\\u200cالمللی صلیب سرخ در کابل برگزار کرده که در آن مسئولات بازداشتگاهاتی ۳۴ ولایت افغانستان شرکت کرده\\u200cاند .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/GEC/Data/Errors/Source\", 'r') as fp:\n",
        "  x = len(fp.readlines())\n",
        "\n",
        "print('Total lines:', x)\n"
      ],
      "metadata": {
        "id": "HStUGHzZxJub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14a2a30-9fc1-4768-9d23-1ca08aa90393"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total lines: 15592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myfile = open(\"/content/drive/MyDrive/GEC/Data/Errors/Source\", \"r\")\n",
        "lines = myfile.readlines()\n",
        "for i in lines[:5]:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQglmv-kt45N",
        "outputId": "8b59afdc-2411-4bee-f7ec-e26b2ad51f8a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "وزارت دادگستری نشستی را به همکاری کمیته بین‌المللی صلیب سرخ در کابل برگزار کرده که در آن مسئولات بازداشتگاهاتی ۳۴ ولایت افغانستان شرکت کرده‌اند .\n",
            "\n",
            "وزارت دادگستری نشستی به همکاری کمیته بین‌المللی صلیب سرخ در کابل را برگزار کرده که در آن مسئولان بازداشتگاه‌های ۳۴ ولایت افغانستان شرکت کرده‌اند .\n",
            "\n",
            "وزارت دادگستری جلسه‌ای با همکاری کمیته بین‌المللی صلیب سرخ در کابل را برگزار کرد ، جایی که مقامات بازداشتگاه افغانستان در آن شرکت کرده‌اند .\n",
            "\n",
            "در عین حال ، به گفته آکسفام ، تعدادی از شرکتاتی بین‌المللی دست‌اندرکار در بخش کشاورزی و فروشگاهاتی زنجیره‌ای عمده به سوداتی بادآورده دست‌یافته‌اند که هزینه اصلی آن را کشاورزات کوچک در کشوراتی فقیر متحمل شده‌اند .\n",
            "\n",
            "در عین حال ، به گفته آکسفام ، تعدادی از شرکتاتی بین‌المللی درگیر در بخش کشاورزی و فروشگاهاتی زنجیره‌ای بزرگ ، سوداتی بادآورده‌ای را به ضرر کشاورزات کوچک در کشوراتی فقیر به دست آورده‌اند .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/GEC/Data/Errors/Target\", 'r') as fp:\n",
        "  x = len(fp.readlines())\n",
        "print('Total lines:', x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox0lhPa0qTBO",
        "outputId": "d1c4c8fa-0f80-46e3-c3f5-fbab36cde904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total lines: 15592\n"
          ]
        }
      ]
    }
  ]
}